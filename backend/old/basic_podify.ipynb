{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from elevenlabs import voices, generate, play, save\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "eleven_api_key = \"34dbe78c2a8c6642fcdc27dbb44c1929\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def podify(topic, style, num_minutes, voice):\n",
    "\n",
    "\n",
    "    num_words = num_minutes*135 # Approximately 135 words per minute with TTS voice (refine this)\n",
    "\n",
    "    audio_voice = voice.split()[0]\n",
    "    summary_lang = \"English\"\n",
    "    if voice.split()[1] == \"(SWE)\":\n",
    "        summary_lang = \"Swedish\"\n",
    "    if voice.split()[1] == \"(IT)\":\n",
    "        summary_lang = \"Italian\"\n",
    "\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    text = wikipedia.run(topic)    \n",
    "\n",
    "    return \"audiofiles/news_eli5.wav\"\n",
    "\n",
    "    # Connect to OpenAI, prompt ChatGPT to summarize the text\n",
    "    chat_model = ChatOpenAI() # Must have set API key as env var\n",
    "    prompt = \"Summarize the following text in {} words. {}. The summary must be in {}. Here's the text: {}\".format(num_words, style, summary_lang, text) \n",
    "    summary = chat_model.predict(prompt)\n",
    "\n",
    "    # Connect to elevenlabs, generate audio\n",
    "    audio = generate(text=summary, voice=audio_voice, model=\"eleven_monolingual_v1\", api_key=eleven_api_key)\n",
    "\n",
    "    # Save audiofile\n",
    "    if not os.path.exists(\"audiofiles\"):\n",
    "        os.makedirs(\"audiofiles\")\n",
    "\n",
    "    # Handling identical file names\n",
    "    file_suffix = 0\n",
    "    while os.path.exists(\"audiofiles/summary_in_{}_mins_{}.wav\".format(num_minutes, file_suffix)):\n",
    "        file_suffix +=1\n",
    "\n",
    "    save(audio, \"audiofiles/summary_in_{}_mins_{}.wav\".format(num_minutes, file_suffix))\n",
    "\n",
    "    return \"audiofiles/summary_in_{}_mins_{}.wav\".format(num_minutes, file_suffix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://1543c61be437f5630b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1543c61be437f5630b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/langchain/utilities/wikipedia.py\", line 33, in validate_environment\n",
      "    import wikipedia\n",
      "ModuleNotFoundError: No module named 'wikipedia'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/routes.py\", line 544, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/route_utils.py\", line 217, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 1553, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 1191, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/utils.py\", line 659, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/22/9tjx5s0x55dcrwblt27w_4ph0000gn/T/ipykernel_18152/3281872101.py\", line 13, in podify\n",
      "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pydantic/v1/main.py\", line 339, in __init__\n",
      "    values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pydantic/v1/main.py\", line 1102, in validate_model\n",
      "    values = validator(cls_, values)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/langchain/utilities/wikipedia.py\", line 38, in validate_environment\n",
      "    raise ImportError(\n",
      "ImportError: Could not import wikipedia python package. Please install it with `pip install wikipedia`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/langchain/utilities/wikipedia.py\", line 33, in validate_environment\n",
      "    import wikipedia\n",
      "ModuleNotFoundError: No module named 'wikipedia'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/routes.py\", line 544, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/route_utils.py\", line 217, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 1553, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 1191, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/utils.py\", line 659, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/22/9tjx5s0x55dcrwblt27w_4ph0000gn/T/ipykernel_18152/3281872101.py\", line 13, in podify\n",
      "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pydantic/v1/main.py\", line 339, in __init__\n",
      "    values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pydantic/v1/main.py\", line 1102, in validate_model\n",
      "    values = validator(cls_, values)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/langchain/utilities/wikipedia.py\", line 38, in validate_environment\n",
      "    raise ImportError(\n",
      "ImportError: Could not import wikipedia python package. Please install it with `pip install wikipedia`.\n"
     ]
    }
   ],
   "source": [
    "demo = gr.Interface(fn=podify, \n",
    "                    inputs=[\n",
    "                            gr.Textbox(label=\"What do you want your podcast to be about?\", placeholder=\"E.g. 'This week's news in AI'\"),\n",
    "                            gr.Textbox(label=\"Do you want to customize the style of your podcast?\", placeholder=\"E.g. 'Explain this to me like I'm five years old'\"),\n",
    "                            gr.Slider(minimum=0.5, maximum=5, step=0.5, label=\"Length\", info = \"Number of minutes of audio summary\"),\n",
    "                            gr.Dropdown([\"Adam (US)\", \"Antoni (IT)\", \"Arnold (US)\", \"Bella (US)\", \"Domi (US)\", \"Elli (US)\", \"Josh (SWE)\", \"Rachel (US)\", \"Sam (US)\"], label=\"Voice\", info=\"Narrator voice and language of podcast\"),\n",
    "                            ],\n",
    "                    outputs= [\"audio\"]\n",
    "                    )\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def podify(topic, style, num_minutes, voice, env=\"prod\"):\n",
    "    num_words = num_minutes*135 # Approximately 135 words per minute with TTS voice (refine this)\n",
    "\n",
    "    audio_voice = voice.split()[0]\n",
    "    summary_lang = \"English\"\n",
    "    if voice.split()[1] == \"(SWE)\":\n",
    "        summary_lang = \"Swedish\"\n",
    "    if voice.split()[1] == \"(IT)\":\n",
    "        summary_lang = \"Italian\"\n",
    "\n",
    "    article_titles = list()\n",
    "    article_body = list()\n",
    "    if(env == \"prod\"):\n",
    "        google_news = gnews.GNews(period='3d', max_results=3)\n",
    "        news_response = google_news.get_news(topic) # En lista av artikelobjekt, article.text, article.title\n",
    "\n",
    "        for news in news_response:\n",
    "            full_article = google_news.get_full_article(news['url'])\n",
    "            article_titles.append(full_article.title)\n",
    "            article_body.append(full_article.text)\n",
    "        with open('./tmp/articles.txt', 'w') as f:   \n",
    "            for i in range(len(article_titles)):\n",
    "                if(i != 0):\n",
    "                    f.write(f\"********\\n\")\n",
    "                f.write(f\"{article_titles[i]}\\n\")\n",
    "                f.write(f\"********\\n\")\n",
    "                f.write(f\"{article_body[i]}\\n\")\n",
    "    elif(env == \"test\"):\n",
    "        with open('./tmp/articles.txt') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = ''.join([line.strip() for line in lines])\n",
    "            lines = lines.split(\"********\")\n",
    "            for i in range(len(lines)):\n",
    "                line = lines[i]\n",
    "                if(i % 2 == 0):\n",
    "                    article_titles.append(line)\n",
    "                else:\n",
    "                    article_body.append(line)\n",
    "    # print(article_titles)\n",
    "    # print(article_body)\n",
    "    combined_text = \"\"\n",
    "    for article in article_body:\n",
    "        combined_text += article\n",
    "\n",
    "    # Get segments from txt by splitting on .\n",
    "    segments =  combined_text.split('.')\n",
    "    # Put the . back in\n",
    "    segments = [segment + '.' for segment in segments]\n",
    "    # Further split by comma\n",
    "    segments = [segment.split(',') for segment in segments]\n",
    "    # Flatten\n",
    "    segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "    print(segments)\n",
    "    return\n",
    "    ### TASK 2 ###\n",
    "    ### Här borde man använda ChatGPT för att gå fråga om artiklarna är relevanta för the user topic ###\n",
    "    ### Hämta alla articles.title, fråga om \"Is this (title) / (first paragraph) relevant to answer (user topic)\"\n",
    "    ### Behöver inte göra en i taget, skicka en lista av titlar/paragrafer i en request - fråga vilka som är relavantas\n",
    "    \n",
    "\n",
    "    # chat_model = ChatOpenAI(model_name='gpt-3.5-turbo') # Must have set API key as env var\n",
    "    # print(article_titles)\n",
    "    # prompt = f'A listener wants to learn about {topic}. This is a list of articles we will give to the listener. {article_titles}. Are these articles relevant based on the title? Return only the titles of all articles that are NOT relevant in the excat same woridng'\n",
    "    # # prompt = \"Summarize the following text in {} words. {}. The summary must be in {}. Here's the text: {}\".format(num_words, style, summary_lang, all_news_text) # The text contains several news articles, separated by this sign '****'.\n",
    "    # test = chat_model.predict(prompt)\n",
    "    # print(test)\n",
    "       \n",
    "\n",
    "    ### TASK 3: Nu har jag bara en jävligt lång kontext-version av GPT, men här vill man köra summeringstekniker av all_news_text\n",
    "    # Connect to OpenAI, prompt ChatGPT to summarize the text\n",
    "    chat_model = ChatOpenAI(model_name='gpt-3.5-turbo-16k') # Must have set API key as env var\n",
    "    prompt = \"Summarize the following text in {} words. {}. The summary must be in {}. Here's the text: {}\".format(num_words, style, summary_lang, all_news_text) # The text contains several news articles, separated by this sign '****'.\n",
    "    summary = chat_model.predict(prompt)\n",
    "\n",
    "    # Connect to elevenlabs, generate audio\n",
    "    audio = generate(text=summary, voice=audio_voice, model=\"eleven_monolingual_v1\", api_key=eleven_api_key)\n",
    "    intro_audio = generate(text='You are listening to an AI-generated podcast on the theme {}, provided to you by PerfectPod AI'.format(topic), voice=audio_voice, model=\"eleven_monolingual_v1\", api_key=eleven_api_key)\n",
    "\n",
    "    # TASK 4, gör detta bättre. Kom på sätt att göra detta effektivt. Lite onödigt kanske med två olika genereringar av ljud? Hur kan man lägga in ljudeffekter bra? (addrrar ljudfiler, overlayar ljud)\n",
    "\n",
    "    # Save audiofile\n",
    "    if not os.path.exists(\"news_audio\"):\n",
    "        os.makedirs(\"news_audio\")\n",
    "\n",
    "    # Handling identical file names\n",
    "    file_suffix = 0\n",
    "    while os.path.exists(\"news_audio/summary_in_{}_mins_{}.wav\".format(num_minutes, file_suffix)):\n",
    "        file_suffix +=1\n",
    "\n",
    "    save(audio, \"news_audio/summary_in_{}_mins_{}.wav\".format(num_minutes, file_suffix))\n",
    "    save(intro_audio, \"news_audio/summary_in_{}_mins_{}_INTRO.wav\".format(num_minutes, file_suffix)) \n",
    "\n",
    "    concatenate_audio_moviepy([\"news_audio/summary_in_{}_mins_{}_INTRO.wav\".format(num_minutes, file_suffix), \"sound_effects/original_transition.wav\"], \"news_audio/summary_in_{}_mins_{}_INTERMED.wav\".format(num_minutes, file_suffix))\n",
    "    concatenate_audio_moviepy([\"news_audio/summary_in_{}_mins_{}_INTERMED.wav\".format(num_minutes, file_suffix), \"news_audio/summary_in_{}_mins_{}.wav\".format(num_minutes, file_suffix)], \"news_audio/summary_in_{}_mins_{}_READY.wav\".format(num_minutes, file_suffix))\n",
    "\n",
    "    return \"news_audio/summary_in_{}_mins_{}_READY.wav\".format(num_minutes, file_suffix)\n",
    "\n",
    "\n",
    "    ### TASK 5: Hur får vi långa poddar. Vi vill ha lång output, och vi måste experimentera med detta.\n",
    "    ### Se hur folk generar hela böcker med ChatGPT, medium. Ex generera templates, generera rekursivt delar. Ha med summeringar av tidigare delar?\n",
    "\n",
    "\n",
    "podify(\"Donald Trump\", \"ELI5\", 1, \"Bella (US)\", \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
